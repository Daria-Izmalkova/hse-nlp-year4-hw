{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измалкова Дарья БКЛ182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "from summa import keywords\n",
    "import RAKE\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собрала корпус из 16 аннотаций и ключевых слов к статьям с ресурса researchgate.net (статьи выбирала рандомные), ссылки к конкретным статьям в столбце \"link\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('nlp_hw1_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>source_keywords</th>\n",
       "      <th>my_keywords</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Social scientists are increasingly interested ...</td>\n",
       "      <td>Environmental values, Animal well-being, Farm ...</td>\n",
       "      <td>farm animals, animal well-being, environmental...</td>\n",
       "      <td>https://www.researchgate.net/publication/35044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The main aim of this article is to start a dis...</td>\n",
       "      <td>Computational social science, Cultural pattern...</td>\n",
       "      <td>social pattern, pattern, sociology</td>\n",
       "      <td>https://www.researchgate.net/publication/35520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Integrating the dynamics and interconnections ...</td>\n",
       "      <td>Asian Drylands Belt, social‐environmental syst...</td>\n",
       "      <td>social-environmental system, SES, Asian Drylan...</td>\n",
       "      <td>https://www.researchgate.net/publication/35518...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Citizen science research has been rapidly expa...</td>\n",
       "      <td>citizen science, political engagement, educati...</td>\n",
       "      <td>youth, citizen science, science efficacy, poli...</td>\n",
       "      <td>https://www.researchgate.net/publication/35519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This research aims to investigate the potentia...</td>\n",
       "      <td>Green advertisement, Consumer empowerment, Cus...</td>\n",
       "      <td>consumer empowerment, organic food, green ads,...</td>\n",
       "      <td>https://www.researchgate.net/publication/35505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Recognition of others' identity through facial...</td>\n",
       "      <td>person perception, face processing, semantic m...</td>\n",
       "      <td>perception, person knowledge, facial features,...</td>\n",
       "      <td>https://www.researchgate.net/publication/35419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Evidence suggests parents of children who expe...</td>\n",
       "      <td>Post-traumatic stress disorder, Prevalence, Ri...</td>\n",
       "      <td>post-traumatic stress disorder, PTSD, parents,...</td>\n",
       "      <td>https://www.researchgate.net/publication/35480...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Current theories of artificial intelligence (A...</td>\n",
       "      <td>emotion, thinking, emotional thinking, AI cons...</td>\n",
       "      <td>artificial intelligence, ai, intelligence, emo...</td>\n",
       "      <td>https://www.researchgate.net/publication/35567...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When we say that what two people feel for each...</td>\n",
       "      <td>true love, ordinary concept, prototype, goodne...</td>\n",
       "      <td>true love, love</td>\n",
       "      <td>https://www.researchgate.net/publication/34869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>During the last decade, intellectual humility ...</td>\n",
       "      <td>Intellectual humility, Measurement, Intellectu...</td>\n",
       "      <td>intellectual humility, definition, measurement</td>\n",
       "      <td>https://www.researchgate.net/publication/35488...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The ending effect describes the phenomenon tha...</td>\n",
       "      <td>ending effect, motivation, risky decision maki...</td>\n",
       "      <td>ending effect, decision-making, risk</td>\n",
       "      <td>https://www.researchgate.net/publication/35482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The past few decades have seen a substantial i...</td>\n",
       "      <td>Artiﬁcial intelligence, Artiﬁcial general inte...</td>\n",
       "      <td>artificial intelligence, ai, artificial genera...</td>\n",
       "      <td>https://www.researchgate.net/publication/35144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Artificial intelligence (AI) technologies and ...</td>\n",
       "      <td>Artifcial intelligence, Machine learning, Anim...</td>\n",
       "      <td>artificial intelligence, ai, animals</td>\n",
       "      <td>https://www.researchgate.net/publication/35373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Consumer resistance is a major barrier to diff...</td>\n",
       "      <td>artificial intelligence, autonomous vehicles, ...</td>\n",
       "      <td>consumer resistance, artificial intelligence, ...</td>\n",
       "      <td>https://www.researchgate.net/publication/35106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The new coronavirus disease 2019 (COVID-19) ha...</td>\n",
       "      <td>COVID-19 pandemic, Artificial intelligence, El...</td>\n",
       "      <td>covid-19 pandemic, covid-19, artificial intell...</td>\n",
       "      <td>https://www.researchgate.net/publication/35547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In the last few years, the Internet of Things,...</td>\n",
       "      <td>food supply chain, computational intelligence,...</td>\n",
       "      <td>food supply chain, fsc, computational intellig...</td>\n",
       "      <td>https://www.researchgate.net/publication/35538...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstract  \\\n",
       "0   Social scientists are increasingly interested ...   \n",
       "1   The main aim of this article is to start a dis...   \n",
       "2   Integrating the dynamics and interconnections ...   \n",
       "3   Citizen science research has been rapidly expa...   \n",
       "4   This research aims to investigate the potentia...   \n",
       "5   Recognition of others' identity through facial...   \n",
       "6   Evidence suggests parents of children who expe...   \n",
       "7   Current theories of artificial intelligence (A...   \n",
       "8   When we say that what two people feel for each...   \n",
       "9   During the last decade, intellectual humility ...   \n",
       "10  The ending effect describes the phenomenon tha...   \n",
       "11  The past few decades have seen a substantial i...   \n",
       "12  Artificial intelligence (AI) technologies and ...   \n",
       "13  Consumer resistance is a major barrier to diff...   \n",
       "14  The new coronavirus disease 2019 (COVID-19) ha...   \n",
       "15  In the last few years, the Internet of Things,...   \n",
       "\n",
       "                                      source_keywords  \\\n",
       "0   Environmental values, Animal well-being, Farm ...   \n",
       "1   Computational social science, Cultural pattern...   \n",
       "2   Asian Drylands Belt, social‐environmental syst...   \n",
       "3   citizen science, political engagement, educati...   \n",
       "4   Green advertisement, Consumer empowerment, Cus...   \n",
       "5   person perception, face processing, semantic m...   \n",
       "6   Post-traumatic stress disorder, Prevalence, Ri...   \n",
       "7   emotion, thinking, emotional thinking, AI cons...   \n",
       "8   true love, ordinary concept, prototype, goodne...   \n",
       "9   Intellectual humility, Measurement, Intellectu...   \n",
       "10  ending effect, motivation, risky decision maki...   \n",
       "11  Artiﬁcial intelligence, Artiﬁcial general inte...   \n",
       "12  Artifcial intelligence, Machine learning, Anim...   \n",
       "13  artificial intelligence, autonomous vehicles, ...   \n",
       "14  COVID-19 pandemic, Artificial intelligence, El...   \n",
       "15  food supply chain, computational intelligence,...   \n",
       "\n",
       "                                          my_keywords  \\\n",
       "0   farm animals, animal well-being, environmental...   \n",
       "1                  social pattern, pattern, sociology   \n",
       "2   social-environmental system, SES, Asian Drylan...   \n",
       "3   youth, citizen science, science efficacy, poli...   \n",
       "4   consumer empowerment, organic food, green ads,...   \n",
       "5   perception, person knowledge, facial features,...   \n",
       "6   post-traumatic stress disorder, PTSD, parents,...   \n",
       "7   artificial intelligence, ai, intelligence, emo...   \n",
       "8                                     true love, love   \n",
       "9      intellectual humility, definition, measurement   \n",
       "10               ending effect, decision-making, risk   \n",
       "11  artificial intelligence, ai, artificial genera...   \n",
       "12               artificial intelligence, ai, animals   \n",
       "13  consumer resistance, artificial intelligence, ...   \n",
       "14  covid-19 pandemic, covid-19, artificial intell...   \n",
       "15  food supply chain, fsc, computational intellig...   \n",
       "\n",
       "                                                 link  \n",
       "0   https://www.researchgate.net/publication/35044...  \n",
       "1   https://www.researchgate.net/publication/35520...  \n",
       "2   https://www.researchgate.net/publication/35518...  \n",
       "3   https://www.researchgate.net/publication/35519...  \n",
       "4   https://www.researchgate.net/publication/35505...  \n",
       "5   https://www.researchgate.net/publication/35419...  \n",
       "6   https://www.researchgate.net/publication/35480...  \n",
       "7   https://www.researchgate.net/publication/35567...  \n",
       "8   https://www.researchgate.net/publication/34869...  \n",
       "9   https://www.researchgate.net/publication/35488...  \n",
       "10  https://www.researchgate.net/publication/35482...  \n",
       "11  https://www.researchgate.net/publication/35144...  \n",
       "12  https://www.researchgate.net/publication/35373...  \n",
       "13  https://www.researchgate.net/publication/35106...  \n",
       "14  https://www.researchgate.net/publication/35547...  \n",
       "15  https://www.researchgate.net/publication/35538...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решила в качестве эталона использовать объединение оригинальных и моих ключевых слов. Оригинальные ключевые слова относятся к стаьям в целом, поэтому иногда там встречаются слова/словосочетания, которые не встречаются (или не частно встречаются) в аннотации. С другой стороны, я не разбираюсь в темах некоторых статей и не уверена, что могу правильно оценить какие слова ключевые, а какие - нет. Также, в оригинальных иногда отсутствуют абревиатуры (есть только термин целиком), которые часто используются в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_union(source_keys, my_keys):\n",
    "    source_set = set(source_keys.lower().split(', '))\n",
    "    my_set = set(my_keys.lower().split(', '))\n",
    "    return ', '.join(list(source_set | my_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords'] = data.apply(lambda x: keyword_union(x.source_keywords,\n",
    "                                                              x.my_keywords), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизирую данные ключевые слова и тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = nlp(text)\n",
    "    lemmas = ' '.join([word.lemma_ for word in tokens\n",
    "                      if word.lemma_ not in stop_words and word.lemma_ != '-PRON-'])\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords_lem'] = data['keywords'].apply(lemmatize)\n",
    "data['abstract_lem'] = data['abstract'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords_lem'] = data['keywords_lem'].apply(lambda x: x.split(' , '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = 'SmartStoplist.txt'\n",
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rake для некоторых лемматизированных текстов не находил ключевые слова (полагаю, что это из-за маленького размера текстов), поэтому воспользуюсь результатами и нелемматизированного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rake_not_lem'] = data['abstract'].apply(lambda x: rake.run(x, maxWords=3, minFrequency=2))\n",
    "data['rake_lem'] = data['abstract_lem'].apply(lambda x: rake.run(x, maxWords=3, minFrequency=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_only(a_list):\n",
    "    keywords = [word[0] for word in a_list]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rake_lem'] = data['rake_lem'].apply(keywords_only)\n",
    "data['rake_not_lem'] = data['rake_not_lem'].apply(keywords_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    []\n",
       "1                                                    []\n",
       "2                                         [social, adb]\n",
       "3                                    [science efficacy]\n",
       "4                                                    []\n",
       "5     [individual similar personality, participant, ...\n",
       "6                                  [trauma factor, 0 %]\n",
       "7                                                    []\n",
       "8                                                    []\n",
       "9                                                    []\n",
       "10                [engage risk, make task, participant]\n",
       "11                                        [friendly ai]\n",
       "12                                                 [ai]\n",
       "13                                                   []\n",
       "14                                                   []\n",
       "15                                                   []\n",
       "Name: rake_lem, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['rake_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [farm animal well-, environmental values, valu...\n",
       "1     [social pattern, concept, patterns, discussion...\n",
       "2        [18 political entities, metric, economic, adb]\n",
       "3     [science efficacy, efficacy, interest, partici...\n",
       "4                      [purchase intentions, consumers]\n",
       "5     [facial features, facial identity, similar per...\n",
       "6     [trauma, parents, ptsd, child, development, cl...\n",
       "7     [ai consciousness, thinking, intelligence, ai,...\n",
       "8                        [true love, picks, real, love]\n",
       "9                    [intellectual humility, framework]\n",
       "10    [decision-making task, risky options, risk lev...\n",
       "11                  [ai, development, humans, question]\n",
       "12    [ai, animals, technologies, application, aspec...\n",
       "13    [brand connection, marketing communications, c...\n",
       "14    [covid-19 pandemic, complex disease, ai techni...\n",
       "15    [ci methods, fsc, problems, ci, stages, method...\n",
       "Name: rake_not_lem, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['rake_not_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [animal welfare, animal agriculture, farm anim...\n",
       "1     [sociology, pattern, cultural pattern, social ...\n",
       "2     [asian drylands belt, social - environmental s...\n",
       "3     [political engagement, political interest, cit...\n",
       "4     [green advertisement, advertisement, green ad,...\n",
       "5     [reverse correlation, person knowledge, face p...\n",
       "6     [prevalence, post - traumatic stress disorder,...\n",
       "7     [intelligence, emotional thinking, cognition, ...\n",
       "8     [love, ordinary concept, prototype, realness, ...\n",
       "9     [measurement, definition, wisdom, jingle - jan...\n",
       "10    [risky decision making, longshot, risk, socioe...\n",
       "11    [control problem, fai, superintelligence, impa...\n",
       "12    [artificial intelligence, animal experiment, a...\n",
       "13    [autonomous vehicle, consumer resistance, self...\n",
       "14    [covid-19, electronic health record, machine l...\n",
       "15    [computational intelligence, food supply chain...\n",
       "Name: keywords_lem, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['keywords_lem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank лучше работает с лемматизированным текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TextRank_not_lem'] = data['abstract'].apply(lambda x: \n",
    "                                                  keywords.keywords(x, \n",
    "                                                                    language='english', \n",
    "                                                                    additional_stopwords=stopwords.words('english'), \n",
    "                                                                    scores=True))\n",
    "data['TextRank_lem'] = data['abstract_lem'].apply(lambda x: \n",
    "                                                  keywords.keywords(x, \n",
    "                                                                    language='english', \n",
    "                                                                    additional_stopwords=stopwords.words('english'), \n",
    "                                                                    scores=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TextRank_lem'] = data['TextRank_lem'].apply(keywords_only)\n",
    "data['TextRank_not_lem'] = data['TextRank_not_lem'].apply(keywords_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [belief farm, value, social, research, importa...\n",
       "1     [useful, discussion social pattern, concept, u...\n",
       "2     [metric, assess, different social, product, pr...\n",
       "3     [science, participation, participate, scientif...\n",
       "4     [effect, effective, ad effectiveness, company,...\n",
       "5     [identity facial, similarly, participant, stud...\n",
       "6     [trauma, ptsd, suggest parent child, factor, d...\n",
       "7     [cognitive, emotional, thinking, human emotion...\n",
       "8     [true love, real, instance, people feel, exper...\n",
       "9     [measure, measurement, research, intellectual ...\n",
       "10    [end, ending, risk, option, motivational, moti...\n",
       "11    [human, development, develop, agi, ai, ethical...\n",
       "12    [animal, application, ai, technology, paper, a...\n",
       "13    [study, influence brand, radical innovation, c...\n",
       "14    [pandemic, disease, covid, clinical, health, r...\n",
       "15    [ci, fsc, problem, stage, method, identify, da...\n",
       "Name: TextRank_lem, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TextRank_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [social scientists, environmental, values, psy...\n",
       "1     [main aim, article, ends, pattern, patterns, l...\n",
       "2     [metric, metrics, social, environmental, produ...\n",
       "3     [science, efficacy, efficacious, scientific, s...\n",
       "4     [research aims, increase, people, online exper...\n",
       "5     [identity, identities, facial, participant, pa...\n",
       "6     [trauma, traumas, effect, effects, ptsd, estim...\n",
       "7     [current theories, theory, social, anthropolog...\n",
       "8           [people feel, use, distinguish, clarifying]\n",
       "9     [research, decade intellectual humility, futur...\n",
       "10    [risk, decision making, previous studies, work...\n",
       "11    [substantial increase, numerous issues, existe...\n",
       "12    [ai, research, debated, debates certain, acade...\n",
       "13    [studies, study, consumer, consumers, brands, ...\n",
       "14    [pandemic, clinical, health, care, disease, co...\n",
       "15    [fsc, ci, problems, data, supply, complexity, ...\n",
       "Name: TextRank_not_lem, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TextRank_not_lem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc, n=0.023):\n",
    "    \n",
    "    doc = clean_text(doc)\n",
    "    total_words = doc.split()\n",
    "    bi_grams = list(nltk.ngrams(total_words, 2))\n",
    "    tri_grams = list(nltk.ngrams(total_words, 3))\n",
    "    bigrams = [' '.join(gram) for gram in bi_grams]\n",
    "    trigrams = [' '.join(gram) for gram in tri_grams]\n",
    "\n",
    "    total_sentences = tokenize.sent_tokenize(doc)\n",
    "\n",
    "# TF    \n",
    "    tf_score1 = tf(total_words)\n",
    "    tf_score2 = tf(bigrams)\n",
    "    tf_score3 = tf(trigrams)\n",
    "\n",
    "    idf_score1 = idf(total_words, total_sentences)\n",
    "    idf_score2 = idf(bigrams, total_sentences)\n",
    "    idf_score3 = idf(trigrams, total_sentences)\n",
    "\n",
    "\n",
    "# TF-IDF   \n",
    "    tf_idf_score = {}\n",
    "    tf_idf_score1 = {key: tf_score1[key] * idf_score1.get(key, 0) for key in tf_score1.keys()}\n",
    "    tf_idf_score2 = {key: tf_score2[key] * idf_score2.get(key, 0) for key in tf_score2.keys()}\n",
    "    tf_idf_score3 = {key: tf_score3[key] * idf_score3.get(key, 0) for key in tf_score3.keys()}\n",
    "    \n",
    "    tf_idf_score.update(tf_idf_score1)\n",
    "    tf_idf_score.update(tf_idf_score2)\n",
    "    tf_idf_score.update(tf_idf_score3)\n",
    "\n",
    "    \n",
    "    return get_top_n(tf_idf_score, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[)(,'\"'=%;’–:\"'\"><\\/—]','',text)\n",
    "    text = text.replace('- ', '')\n",
    "    text = re.sub('  ',' ', text) \n",
    "    text = text.replace(' .','.')\n",
    "    text = text.replace(' ?','.')\n",
    "    text = text.replace(' !','.')\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(dict_elem, n=0.02):\n",
    "    res = {}\n",
    "    for word, score in dict_elem.items():\n",
    "        if score > n:\n",
    "            res.update({word: score})\n",
    "    return sorted(res.items(), key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(word, sentences): \n",
    "    final = [all([w in x for w in word]) for x in sentences] \n",
    "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
    "    return int(len(sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(words):\n",
    "    tf_score = {}\n",
    "    for word in words:\n",
    "        word = word.replace('.','')\n",
    "        if word not in stop_words:\n",
    "            if word in tf_score:\n",
    "                tf_score[word] += 1\n",
    "            else:\n",
    "                tf_score[word] = 1\n",
    "    tf_score.update((x, y/len(words)) for x, y in tf_score.items())\n",
    "    return tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(words, sentences):\n",
    "    idf_score = {}\n",
    "    for word in words:\n",
    "        word = word.replace('.','')\n",
    "        if word not in stop_words:\n",
    "            if word in idf_score:\n",
    "                idf_score[word] = check_sent(word, sentences)\n",
    "            else:\n",
    "                idf_score[word] = 1\n",
    "\n",
    "    idf_score.update((x, math.log((len(sentences))/y)) for x, y in idf_score.items())\n",
    "    return idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tfidf_not_lem'] = data['abstract'].apply(tf_idf)\n",
    "data['tfidf_lem'] = data['abstract_lem'].apply(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tfidf_lem'] = data['tfidf_lem'].apply(keywords_only)\n",
    "data['tfidf_not_lem'] = data['tfidf_not_lem'].apply(keywords_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf_Idf тоже лучше работает с лемматизированным текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [farm, well, farm animal]\n",
       "1                                                    []\n",
       "2                                               [index]\n",
       "3     [project, external reward motivation, external...\n",
       "4                                                    []\n",
       "5     [individual, individual similar personality, i...\n",
       "6                                           [95 ci, 95]\n",
       "7         [thinking, symbolic representation, symbolic]\n",
       "8     [pick love, pick, love, say two people, two pe...\n",
       "9     [framework, last decade intellectual, decade i...\n",
       "10                                                   []\n",
       "11                                           [question]\n",
       "12    [research, artificial intelligence ai, intelli...\n",
       "13                                                   []\n",
       "14                                                   []\n",
       "15                                                   []\n",
       "Name: tfidf_lem, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tfidf_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             []\n",
       "1             []\n",
       "2             []\n",
       "3      [project]\n",
       "4             []\n",
       "5             []\n",
       "6             []\n",
       "7     [thinking]\n",
       "8             []\n",
       "9     [has been]\n",
       "10            []\n",
       "11            []\n",
       "12            []\n",
       "13            []\n",
       "14            []\n",
       "15            []\n",
       "Name: tfidf_not_lem, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tfidf_not_lem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Набираю список шаблонов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_list(a_list):\n",
    "    new_list = [lemmatize(word) for word in a_list]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rake_not_lem'] = data['rake_not_lem'].apply(lemm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [farm, well, farm animal]\n",
       "1                                                    []\n",
       "2                                               [index]\n",
       "3     [project, external reward motivation, external...\n",
       "4                                                    []\n",
       "5     [individual, individual similar personality, i...\n",
       "6                                           [95 ci, 95]\n",
       "7         [thinking, symbolic representation, symbolic]\n",
       "8     [pick love, pick, love, say two people, two pe...\n",
       "9     [framework, last decade intellectual, decade i...\n",
       "10                                                   []\n",
       "11                                           [question]\n",
       "12    [research, artificial intelligence ai, intelli...\n",
       "13                                                   []\n",
       "14                                                   []\n",
       "15                                                   []\n",
       "Name: tfidf_lem, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tfidf_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = []\n",
    "filter_set = set()\n",
    "for key_list in data['keywords']:\n",
    "\n",
    "    for key_word in key_list.split(', '):\n",
    "        pos_tags = []\n",
    "        tokens = nlp(key_word)\n",
    "        for token in tokens:\n",
    "            pos_tags.append(token.pos_)\n",
    "        pos_filter = '+'.join(pos_tags)\n",
    "        filters.append(pos_filter)\n",
    "\n",
    "filter_counts = Counter(filters)\n",
    "\n",
    "for key, value in filter_counts.items():\n",
    "    if value > 2:\n",
    "        filter_set.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ+NOUN', 'NOUN', 'NOUN+NOUN', 'NOUN+PROPN', 'PROPN', 'PROPN+PROPN', 'VERB'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter(filters, keywords):\n",
    "    filtered = []\n",
    "    for keyword in keywords:\n",
    "        pos_tags = []\n",
    "        tokens = nlp(keyword)\n",
    "        for token in tokens:\n",
    "            pos_tags.append(token.pos_)\n",
    "        pos = '+'.join(pos_tags)\n",
    "        if pos in filters:\n",
    "            filtered.append(keyword)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['rake_not_lem_filtered'] = data['rake_not_lem'].apply(lambda x: apply_filter(filter_set, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TextRank_lem_filtered'] = data['TextRank_lem'].apply(lambda x: apply_filter(filter_set, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tfidf_lem_filtered'] = data['tfidf_lem'].apply(lambda x: apply_filter(filter_set, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(real, predicted):\n",
    "    prec = precision(real, predicted)\n",
    "    rec = recall(real, predicted)\n",
    "    if prec == 'No keywords were extracted':\n",
    "        return 'No keywords were extracted'\n",
    "    elif prec == 0:\n",
    "        return 'There were no True Positives'\n",
    "    else:\n",
    "        f_score = (prec * rec)/(prec + rec)\n",
    "        return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(real, predicted):\n",
    "    TP = 0\n",
    "    for word in real:\n",
    "        if word in set(predicted):\n",
    "            TP +=1\n",
    "    if len(predicted) != 0:\n",
    "        precision = TP / len(predicted)\n",
    "        return precision\n",
    "    else:\n",
    "        return 'No keywords were extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(real, predicted):\n",
    "    TP = 0\n",
    "    for word in real:\n",
    "        if word in set(predicted):\n",
    "            TP +=1\n",
    "    if len(predicted) != 0:\n",
    "        recall = TP / len(real)\n",
    "        return recall\n",
    "    else:\n",
    "        return 'No keywords were extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rake_not_filtered_f_score'] = data.apply(lambda x: f_score(x.keywords_lem,\n",
    "                                                              x.rake_not_lem), axis=1)\n",
    "data['TextRank_not_filtered_f_score'] = data.apply(lambda x: f_score(x.keywords_lem,\n",
    "                                                              x.TextRank_lem), axis=1)\n",
    "data['tfidf_not_filtered_f_score'] = data.apply(lambda x: f_score(x.keywords_lem,\n",
    "                                                              x.tfidf_lem), axis=1)\n",
    "data['rake_not_filtered_precision'] = data.apply(lambda x: precision(x.keywords_lem,\n",
    "                                                              x.rake_not_lem), axis=1)\n",
    "data['TextRank_not_filtered_precision'] = data.apply(lambda x: precision(x.keywords_lem,\n",
    "                                                              x.TextRank_lem), axis=1)\n",
    "data['tfidf_not_filtered_precision'] = data.apply(lambda x: precision(x.keywords_lem,\n",
    "                                                              x.tfidf_lem), axis=1)\n",
    "data['rake_not_filtered_recall'] = data.apply(lambda x: recall(x.keywords_lem,\n",
    "                                                              x.rake_not_lem), axis=1)\n",
    "data['TextRank_not_filtered_recall'] = data.apply(lambda x: recall(x.keywords_lem,\n",
    "                                                              x.TextRank_lem), axis=1)\n",
    "data['tfidf_not_filtered_recall'] = data.apply(lambda x: recall(x.keywords_lem,\n",
    "                                                              x.tfidf_lem), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rake_filtered_f_score'] = data.apply(lambda x: f_score(x.keywords_lem,\n",
    "                                                              x.rake_not_lem_filtered), axis=1)\n",
    "data['TextRank_filtered_f_score'] = data.apply(lambda x: f_score(x.keywords_lem,\n",
    "                                                              x.TextRank_lem_filtered), axis=1)\n",
    "data['tfidf_filtered_f_score'] = data.apply(lambda x: f_score(x.keywords_lem,\n",
    "                                                              x.tfidf_lem_filtered), axis=1)\n",
    "data['rake_filtered_precision'] = data.apply(lambda x: precision(x.keywords_lem,\n",
    "                                                              x.rake_not_lem_filtered), axis=1)\n",
    "data['TextRank_filtered_precision'] = data.apply(lambda x: precision(x.keywords_lem,\n",
    "                                                              x.TextRank_lem_filtered), axis=1)\n",
    "data['tfidf_filtered_precision'] = data.apply(lambda x: precision(x.keywords_lem,\n",
    "                                                              x.tfidf_lem_filtered), axis=1)\n",
    "data['rake_filtered_recall'] = data.apply(lambda x: recall(x.keywords_lem,\n",
    "                                                              x.rake_not_lem_filtered), axis=1)\n",
    "data['TextRank_filtered_recall'] = data.apply(lambda x: recall(x.keywords_lem,\n",
    "                                                              x.TextRank_lem_filtered), axis=1)\n",
    "data['tfidf_filtered_recall'] = data.apply(lambda x: recall(x.keywords_lem,\n",
    "                                                              x.tfidf_lem_filtered), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление фильтра в некоторых случаях улучшает f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rake_not_filtered_f_score</th>\n",
       "      <th>rake_filtered_f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0909091</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0833333</td>\n",
       "      <td>0.0833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0526316</td>\n",
       "      <td>0.0555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0869565</td>\n",
       "      <td>0.0869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rake_not_filtered_f_score         rake_filtered_f_score\n",
       "0                       0.166667                      0.181818\n",
       "1                       0.214286                      0.214286\n",
       "2                      0.0909091                         0.125\n",
       "3                      0.0833333                     0.0833333\n",
       "4                       0.111111                      0.111111\n",
       "5                       0.117647                      0.133333\n",
       "6                         0.1875                      0.214286\n",
       "7                       0.277778                      0.235294\n",
       "8                       0.181818                           0.2\n",
       "9                          0.125                         0.125\n",
       "10                     0.0526316                     0.0555556\n",
       "11                        0.0625                     0.0666667\n",
       "12                      0.117647                      0.117647\n",
       "13  There were no True Positives  There were no True Positives\n",
       "14                      0.166667                      0.181818\n",
       "15                     0.0869565                     0.0869565"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['rake_not_filtered_f_score', 'rake_filtered_f_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextRank_not_filtered_f_score</th>\n",
       "      <th>TextRank_filtered_f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0434783</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0526316</td>\n",
       "      <td>0.0666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0526316</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0454545</td>\n",
       "      <td>0.0555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0714286</td>\n",
       "      <td>0.0769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0769231</td>\n",
       "      <td>0.0909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0512821</td>\n",
       "      <td>0.0606061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TextRank_not_filtered_f_score     TextRank_filtered_f_score\n",
       "0   There were no True Positives  There were no True Positives\n",
       "1   There were no True Positives  There were no True Positives\n",
       "2                      0.0434783                        0.0625\n",
       "3                      0.0526316                     0.0666667\n",
       "4                      0.0526316                        0.0625\n",
       "5   There were no True Positives  There were no True Positives\n",
       "6                      0.0454545                     0.0555556\n",
       "7                      0.0714286                     0.0769231\n",
       "8                      0.0769231                     0.0909091\n",
       "9                       0.133333                      0.153846\n",
       "10                      0.105263                      0.111111\n",
       "11                      0.111111                      0.130435\n",
       "12                      0.117647                         0.125\n",
       "13                          0.05                     0.0526316\n",
       "14                      0.030303                     0.0384615\n",
       "15                     0.0512821                     0.0606061"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['TextRank_not_filtered_f_score', 'TextRank_filtered_f_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_not_filtered_f_score</th>\n",
       "      <th>tfidf_filtered_f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No keywords were extracted</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00913242</td>\n",
       "      <td>0.0289855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No keywords were extracted</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0145631</td>\n",
       "      <td>0.0526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00529101</td>\n",
       "      <td>0.0140845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>No keywords were extracted</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>There were no True Positives</td>\n",
       "      <td>There were no True Positives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00431034</td>\n",
       "      <td>0.0140845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>No keywords were extracted</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>No keywords were extracted</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>No keywords were extracted</td>\n",
       "      <td>No keywords were extracted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidf_not_filtered_f_score        tfidf_filtered_f_score\n",
       "0                       0.111111                         0.125\n",
       "1     No keywords were extracted    No keywords were extracted\n",
       "2   There were no True Positives  There were no True Positives\n",
       "3                     0.00913242                     0.0289855\n",
       "4     No keywords were extracted    No keywords were extracted\n",
       "5   There were no True Positives  There were no True Positives\n",
       "6   There were no True Positives    No keywords were extracted\n",
       "7   There were no True Positives  There were no True Positives\n",
       "8                      0.0145631                     0.0526316\n",
       "9                     0.00529101                     0.0140845\n",
       "10    No keywords were extracted    No keywords were extracted\n",
       "11  There were no True Positives  There were no True Positives\n",
       "12                    0.00431034                     0.0140845\n",
       "13    No keywords were extracted    No keywords were extracted\n",
       "14    No keywords were extracted    No keywords were extracted\n",
       "15    No keywords were extracted    No keywords were extracted"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['tfidf_not_filtered_f_score', 'tfidf_filtered_f_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я заметила, что предсказанные алгоритмами ключевые слова часто близки к эталону, но не совпадает деление биграмм/триграмм. Для наглядности и для того, чтобы сравнить методы между собой, решила посмотреть какие отдельные слова, вообще, сопадают в эталонных и предсказанных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(real, predicted):\n",
    "    real_string = ' '.join(real)\n",
    "    pred_string = ' '.join(predicted)\n",
    "    return set(real_string.split()) & set(pred_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TextRank_sims'] =  data.apply(lambda x: similarity(x.keywords_lem,\n",
    "                                                              x.TextRank_lem), axis=1)\n",
    "data['rake_sims'] =  data.apply(lambda x: similarity(x.keywords_lem,\n",
    "                                                              x.rake_not_lem), axis=1)\n",
    "data['tfidf_sims'] =  data.apply(lambda x: similarity(x.keywords_lem,\n",
    "                                                              x.tfidf_lem), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  {value, environmental, belief, farm}\n",
       "1                          {social, sociology, pattern}\n",
       "2                          {environmental, social, adb}\n",
       "3                            {efficacy, science, youth}\n",
       "4         {consumer, green, empowerment, intention, ad}\n",
       "5               {identity, facial, person, correlation}\n",
       "6        {traumatic, parent, post, ptsd, factor, child}\n",
       "7     {intelligence, cognition, emotion, ai, computi...\n",
       "8                              {experiment, true, love}\n",
       "9                 {humility, measurement, intellectual}\n",
       "10                    {end, risk, motivation, decision}\n",
       "11         {intelligence, control, ai, artificial, agi}\n",
       "12                                         {ai, animal}\n",
       "13      {consumer, innovation, resistance, self, brand}\n",
       "14                               {pandemic, health, ai}\n",
       "15                                    {ci, method, fsc}\n",
       "Name: TextRank_sims, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TextRank_sims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          {value, environmental, farm, belief, animal}\n",
       "1                          {social, sociology, pattern}\n",
       "2                                                 {adb}\n",
       "3                         {efficacy, science, interest}\n",
       "4                       {purchase, intention, consumer}\n",
       "5         {perception, face, facial, feature, identity}\n",
       "6                                 {ptsd, parent, child}\n",
       "7     {intelligence, emotion, ai, think, consciousness}\n",
       "8                                          {true, love}\n",
       "9                              {humility, intellectual}\n",
       "10              {risky, end, risk, effect, decision, -}\n",
       "11                                                 {ai}\n",
       "12                                         {ai, animal}\n",
       "13                                  {brand, connection}\n",
       "14                             {pandemic, covid-19, ai}\n",
       "15                                    {ci, method, fsc}\n",
       "Name: rake_sims, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['rake_sims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  {farm, well, animal}\n",
       "1                                                    {}\n",
       "2                                                    {}\n",
       "3     {efficacy, citizen, political, education, scie...\n",
       "4                                                    {}\n",
       "5                                                    {}\n",
       "6                                                    {}\n",
       "7                                            {thinking}\n",
       "8           {concept, ordinary, true, experiment, love}\n",
       "9     {definition, humility, measurement, intellectual}\n",
       "10                                                   {}\n",
       "11                                                   {}\n",
       "12    {conservation, intelligence, ai, artificial, a...\n",
       "13                                                   {}\n",
       "14                                                   {}\n",
       "15                                                   {}\n",
       "Name: tfidf_sims, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tfidf_sims']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом автоматические методы плохо справились, думаю, что в первую очередь это связано с тем, что я использовала очень маленькие тексты + оригинальные ключевые слова относились к статье в целом, а не только к аннотации. Алгоритмы воспринимают абревиатуры и их расшифровки как отдельные слова, а живые люди, конечно, нет. Многие важные детали в текстах не являются частотными - например, информация о методах и типах экспериментов.\n",
    "Если сравнить методы между собой, то сильно хуже других справился Tf-Idf, TextRank и Rake ошибались в разных местах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
